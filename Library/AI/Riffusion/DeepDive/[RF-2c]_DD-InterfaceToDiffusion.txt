

ðŸ§  The Producer.ai Signal Flow1. The Translator LayerThe AI interface acts as a Prompt Engineer. It doesn't just pass your text; it hydrates it with metadata.Intent Parsing: Converts "Make it heavier" into weirdness: 0.8 + text: "distorted, industrial, aggressive".Implicit Key/BPM: If not specified, the AI guesses based on the "vibe" or context, which often causes the "fighting" you experienced during the remix.2. Spectrogram "Painting" vs. SamplingUnlike a DAW (Ableton/FL), the backend uses Latent Diffusion.The Problem: It doesn't "hear" a guitar; it sees a shape in a spectrogram.The Failure: When cover_strength is > 0.3, the model attempts to "re-denoise" that shape. If your prompt says "Grime," it fills the guitar's "shape" with "Grime pixels" (synths), destroying the organic texture.The Fix: Use riff_split_stems_v2 first. Only run the remix on the Drum/Bass stems while keeping the Other stem (Guitar) untouched or processed only via apply_audio_effect.3. Temporal Masking & Vibe Injection (The Complex Hack)The riff_create_custom and riff_create_cover tools allow for powerful temporal control via the sound_prompts array.A. Temporal Masking (time_start / time_end)Function: Allows the user to define a "timeline" where the AI must switch between stylistic directives.Mechanism: The diffusion model must mathematically blend the end of one spectrogram "painting" into the start of the next at the specified time marker. This is how complex transitions are forced.B. Vibe Injection (vibe: {audio_input: {id}})Function: Injecting the "sonic signature" of a source audio file (often an uploaded stem) into a new generation without copying its melody.Mechanism: When text: null is used alongside the vibe object and a low strength (e.g., 0.3), the model focuses on borrowing the texture, air, mix quality, or spectral balance of the audio UUID, while ignoring the core melodic content. This is essential for maintaining acoustic fidelity during a style swap.